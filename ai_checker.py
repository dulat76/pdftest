"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ò–ò
–° –ò–ù–¢–ï–ì–†–ê–¶–ò–ï–ô –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø –í POSTGRESQL
–ò–°–ü–†–ê–í–õ–ï–ù–ê –ö–û–î–ò–†–û–í–ö–ê UTF-8
"""

import os
import json
from typing import List, Dict, Optional
import requests
from dataclasses import dataclass

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞
try:
    from ai_cache import cache_manager
    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ–¥—É–ª—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

@dataclass
class AICheckResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –ò–ò"""
    is_correct: bool
    confidence: float  # 0.0 - 1.0
    explanation: str
    ai_provider: str
    from_cache: bool = False  # –ù–æ–≤–æ–µ –ø–æ–ª–µ: –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –Ω–µ—Ç


class AIAnswerChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –ò–ò —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, provider: str = "gemini", api_key: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä—â–∏–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            provider: "groq", "gemini", "huggingface", "cohere", –∏–ª–∏ "ollama"
            api_key: API –∫–ª—é—á (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è). –î–ª—è ollama –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.
        """
        self.provider = provider.lower()
        
        # –î–ª—è Ollama API –∫–ª—é—á –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if self.provider == "ollama":
            self.api_key = None
        else:
            self.api_key = api_key if api_key else self._get_api_key_from_env()
            
            if not self.api_key:
                raise ValueError(f"API –∫–ª—é—á –¥–ª—è {provider} –Ω–µ –Ω–∞–π–¥–µ–Ω. "
                               f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è {self._get_env_var_name()} –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–π—Ç–µ –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é.")
    
    def _get_env_var_name(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è API –∫–ª—é—á–∞"""
        env_vars = {
            "groq": "GROQ_API_KEY",
            "gemini": "GOOGLE_API_KEY",
            "huggingface": "HUGGINGFACE_API_KEY",
            "cohere": "COHERE_API_KEY"
        }
        return env_vars.get(self.provider, "AI_API_KEY")
    
    def _get_api_key_from_env(self) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        return os.getenv(self._get_env_var_name())
    
    def check_answer(self, 
                     student_answer: str, 
                     correct_variants: List[str],
                     question_context: str = "",
                     system_prompt: Optional[str] = None,
                     model_name: Optional[str] = None) -> AICheckResult:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞
        
        Args:
            student_answer: –û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞
            correct_variants: –°–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
            question_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            system_prompt: –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            AICheckResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑ –∫—ç—à–∞)
        """
        from ai_config import AIConfig
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        model_to_use = model_name or AIConfig.GEMINI_MODEL
        
        # 1. –ü–†–û–í–ï–†–ö–ê –ö–≠–®–ê (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –≤–∫–ª—é—á–µ–Ω)
        if CACHE_AVAILABLE and AIConfig.CACHE_AI_RESPONSES:
            cached_result = cache_manager.get_cached_result(
                student_answer=student_answer,
                correct_variants=correct_variants,
                question_context=question_context,
                ai_model=model_to_use
            )
            
            if cached_result:
                print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è: '{student_answer}'")
                return AICheckResult(
                    is_correct=cached_result['is_correct'],
                    confidence=cached_result['confidence'],
                    explanation=cached_result['explanation'],
                    ai_provider=cached_result['ai_provider'],
                    from_cache=True
                )
        
        # 2. –í–´–ó–û–í –ò–ò (–µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫—ç—à–µ)
        if self.provider == "groq":
            result = self._check_with_groq(student_answer, correct_variants, question_context, system_prompt)
        elif self.provider == "gemini":
            result = self._check_with_gemini(student_answer, correct_variants, question_context, system_prompt, model_to_use)
        elif self.provider == "huggingface":
            result = self._check_with_huggingface(student_answer, correct_variants, question_context)
        elif self.provider == "cohere":
            result = self._check_with_cohere(student_answer, correct_variants, question_context, system_prompt)
        elif self.provider == "ollama":
            result = self._check_with_ollama(student_answer, correct_variants, question_context, system_prompt, model_to_use)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider}")
        
        # 3. –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ö–≠–® (–µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏ –∫—ç—à –¥–æ—Å—Ç—É–ø–µ–Ω)
        if CACHE_AVAILABLE and AIConfig.CACHE_AI_RESPONSES and not result.from_cache:
            cache_saved = cache_manager.save_to_cache(
                student_answer=student_answer,
                correct_variants=correct_variants,
                question_context=question_context,
                ai_provider=result.ai_provider,
                ai_model=model_to_use,
                is_correct=result.is_correct,
                confidence=result.confidence,
                explanation=result.explanation,
                ttl=AIConfig.CACHE_DURATION
            )
            
            if cache_saved:
                print(f"üíæ –û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à: '{student_answer}'")
            else:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à: '{student_answer}'")
        
        return result
    
    def _build_prompt(self, student_answer: str, correct_variants: List[str], 
                     question_context: str = "") -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–∞"""
        correct_answers_str = "\n".join([f"- {v}" for v in correct_variants])
        
        return f"""–ü—Ä–æ–≤–µ—Ä—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º.

–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞: {question_context or "–ù–µ —É–∫–∞–∑–∞–Ω"}

–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞:
{correct_answers_str}

–û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞: "{student_answer}"

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
{{"is_correct": true/false, "confidence": —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 100, "explanation": "–∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ"}}"""
    
    def _check_with_groq(self, student_answer: str, correct_variants: List[str], 
                        question_context: str = "",
                        system_prompt: Optional[str] = None) -> AICheckResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ Groq API"""
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        user_prompt = self._build_prompt(student_answer, correct_variants, question_context)
        
        data = {
            "model": "llama-3.1-8b-instant",
            "messages": [
                {"role": "system", "content": system_prompt or "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ—Ç–≤–µ—Ç–æ–≤. –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,
            "max_tokens": 200
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            json_result = self._extract_json(content)
            
            return AICheckResult(
                is_correct=json_result.get('is_correct', False),
                confidence=json_result.get('confidence', 0) / 100.0,
                explanation=json_result.get('explanation', '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ'),
                ai_provider='groq',
                from_cache=False
            )
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Groq API: {e}")
            return self._fallback_check(student_answer, correct_variants, error_message=str(e))
    
    def _build_gemini_request_body(self, student_answer: str, correct_variants: List[str],
                                   question_context: str, system_prompt: str,
                                   generation_config: Dict) -> Dict:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è Gemini API —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π"""
        
        correct_answers_str = "\n".join([f"- {v}" for v in correct_variants])
        
        # –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        user_prompt_text = f"""–ü—Ä–æ–≤–µ—Ä—å –æ—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–í–æ–ø—Ä–æ—Å/–ö–æ–Ω—Ç–µ–∫—Å—Ç: {question_context or "–ù–µ —É–∫–∞–∑–∞–Ω"}

–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã:
{correct_answers_str}

–û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞: "{student_answer}"

–ö—Ä–∏—Ç–µ—Ä–∏–∏:
- –£—á–∏—Ç—ã–≤–∞–π —Å–∏–Ω–æ–Ω–∏–º—ã, –æ–ø–µ—á–∞—Ç–∫–∏, –ø–∞–¥–µ–∂–∏
- –ë—É–¥—å –ª–æ—è–ª–µ–Ω –µ—Å–ª–∏ —Å—É—Ç—å –≤–µ—Ä–Ω–∞
- VR = virtual reality (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–æ–ø—É—Å—Ç–∏–º—ã)
- –ò—Å—Ç–∏–Ω–∞/–í–µ—Ä–Ω–æ/True - —Å–∏–Ω–æ–Ω–∏–º—ã
- –õ–æ–∂—å/–ù–µ –≤–µ—Ä–Ω–æ/False - —Å–∏–Ω–æ–Ω–∏–º—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Ç–æ–ª—å–∫–æ JSON, –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ):
{{"is_correct": true, "confidence": 95, "explanation": "–∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ"}}"""
        
        # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è JSON –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        json_generation_config = {
            "temperature": 0.0,
            "top_p": 0.8,
            "top_k": 10,
            "max_output_tokens": 100,
            "candidate_count": 1
        }
        
        request_body = {
            "contents": [{
                "parts": [{"text": user_prompt_text}]
            }],
            "generationConfig": json_generation_config
        }
        
        return request_body

    def _check_with_gemini(self, student_answer: str, correct_variants: List[str],
                          question_context: str = "",
                          system_prompt: Optional[str] = None,
                          model_name: Optional[str] = None) -> AICheckResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ Google Gemini API —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏"""
        from ai_config import AIConfig

        model_to_use = model_name or AIConfig.GEMINI_MODEL
        url = f"https://generativelanguage.googleapis.com/v1/models/{model_to_use}:generateContent?key={self.api_key}"
        
        data = self._build_gemini_request_body(
            student_answer, correct_variants, question_context,
            system_prompt or AIConfig.VERIFICATION_PROMPT_TEMPLATE,
            AIConfig.GENERATION_CONFIG
        )
        
        try:
            # –ö–†–ò–¢–ò–ß–ù–û: –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8
            headers = {
                "Content-Type": "application/json; charset=utf-8"
            }
            
            response = requests.post(
                url, 
                json=data, 
                headers=headers,
                timeout=15
            )
            
            # –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –æ—Ç–≤–µ—Ç–∞
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
            result = response.json()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ candidates
            if 'candidates' not in result or not result['candidates']:
                error_msg = "Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç"
                if 'promptFeedback' in result:
                    error_msg += f": {result['promptFeedback']}"
                raise Exception(error_msg)
            
            content = result['candidates'][0]['content']['parts'][0]['text'].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            json_result = self._extract_json(content)
            
            return AICheckResult(
                is_correct=json_result.get('is_correct', False),
                confidence=json_result.get('confidence', 0) / 100.0,
                explanation=json_result.get('explanation', '–ù–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –æ—Ç AI'),
                ai_provider='gemini',
                from_cache=False
            )
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Gemini API: {e}")
            return self._fallback_check(student_answer, correct_variants, error_message=str(e))
    
    def _check_with_huggingface(self, student_answer: str, correct_variants: List[str],
                               question_context: str = "") -> AICheckResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ HuggingFace API"""
        url = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        data = {
            "inputs": student_answer,
            "parameters": {"candidate_labels": ["correct", "incorrect"]}
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            result = response.json()
            
            is_correct = result['labels'][0] == 'correct'
            confidence = result['scores'][0]
            
            return AICheckResult(
                is_correct=is_correct,
                confidence=confidence,
                explanation=f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {result['labels'][0]} ({confidence*100:.1f}%)",
                ai_provider='huggingface',
                from_cache=False
            )
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ HuggingFace API: {e}")
            return self._fallback_check(student_answer, correct_variants, error_message=str(e))
    
    def _check_with_cohere(self, student_answer: str, correct_variants: List[str],
                          question_context: str = "",
                          system_prompt: Optional[str] = None) -> AICheckResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ Cohere API"""
        url = "https://api.cohere.ai/v1/generate"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json; charset=utf-8"
        }
        
        user_prompt = self._build_prompt(student_answer, correct_variants, question_context)
        
        data = {
            "model": "command-light",
            "prompt": f"{system_prompt}\n\n{user_prompt}" if system_prompt else user_prompt,
            "max_tokens": 200,
            "temperature": 0.1
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=10)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            result = response.json()
            content = result['generations'][0]['text'].strip()
            
            json_result = self._extract_json(content)
            
            return AICheckResult(
                is_correct=json_result.get('is_correct', False),
                confidence=json_result.get('confidence', 0) / 100.0,
                explanation=json_result.get('explanation', '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ'),
                ai_provider='cohere',
                from_cache=False
            )
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Cohere API: {e}")
            return self._fallback_check(student_answer, correct_variants, error_message=str(e))
    
    def _check_with_ollama(self, student_answer: str, correct_variants: List[str],
                           question_context: str = "",
                           system_prompt: Optional[str] = None,
                           model_name: str = "qwen2.5:1.5b") -> AICheckResult:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é Ollama –º–æ–¥–µ–ª—å"""
        # –ü–æ–ª—É—á–∞–µ–º URL Ollama –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ Gemini
        correct_answers_str = "\n".join([f"- {v}" for v in correct_variants])
        
        user_prompt = f"""–ü—Ä–æ–≤–µ—Ä—å –æ—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–í–æ–ø—Ä–æ—Å/–ö–æ–Ω—Ç–µ–∫—Å—Ç: {question_context or "–ù–µ —É–∫–∞–∑–∞–Ω"}

–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã:
{correct_answers_str}

–û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞: "{student_answer}"

–ö—Ä–∏—Ç–µ—Ä–∏–∏:
- –£—á–∏—Ç—ã–≤–∞–π —Å–∏–Ω–æ–Ω–∏–º—ã, –æ–ø–µ—á–∞—Ç–∫–∏, –ø–∞–¥–µ–∂–∏
- –ë—É–¥—å –ª–æ—è–ª–µ–Ω –µ—Å–ª–∏ —Å—É—Ç—å –≤–µ—Ä–Ω–∞
- VR = virtual reality (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–æ–ø—É—Å—Ç–∏–º—ã)
- –ò—Å—Ç–∏–Ω–∞/–í–µ—Ä–Ω–æ/True - —Å–∏–Ω–æ–Ω–∏–º—ã
- –õ–æ–∂—å/–ù–µ –≤–µ—Ä–Ω–æ/False - —Å–∏–Ω–æ–Ω–∏–º—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (—Ç–æ–ª—å–∫–æ JSON, –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ):
{{"is_correct": true, "confidence": 95, "explanation": "–∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ"}}"""
        
        try:
            url = f"{ollama_url}/api/generate"
            
            response = requests.post(
                url,
                json={
                    "model": model_name,
                    "prompt": user_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 200,
                        "top_p": 0.8,
                        "top_k": 10
                    }
                },
                timeout=30  # Ollama –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ CPU
            )
            
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            result = response.json()
            content = result.get("response", "").strip()
            
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_result = self._extract_json(content)
            
            return AICheckResult(
                is_correct=json_result.get('is_correct', False),
                confidence=json_result.get('confidence', 0) / 100.0,
                explanation=json_result.get('explanation', '–ù–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –æ—Ç AI'),
                ai_provider='ollama',
                from_cache=False
            )
            
        except requests.exceptions.ConnectionError:
            error_msg = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama –ø–æ –∞–¥—Ä–µ—Å—É {ollama_url}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω."
            print(f"‚ùå –û—à–∏–±–∫–∞ Ollama: {error_msg}")
            return self._fallback_check(student_answer, correct_variants, error_message=error_msg)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Ollama: {e}")
            return self._fallback_check(student_answer, correct_variants, error_message=str(e))
    
    def _extract_json(self, text: str) -> Dict:
        """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π UTF-8"""
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ç–µ–∫—Å—Ç –≤ UTF-8
            if isinstance(text, bytes):
                text = text.decode('utf-8')
            
            # –£–±–∏—Ä–∞–µ–º markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            text = text.replace('```json', '').replace('```', '').strip()
            
            # –£–º–Ω—ã–π –ø–æ–∏—Å–∫ JSON –±–ª–æ–∫–∞
            start = text.find('{')
            end = text.rfind('}') + 1
            
            if start != -1 and end != 0:
                json_str = text[start:end]
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–≤ Python 3.x json.loads –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å unicode)
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
                    
                    # 1. –ó–∞–º–µ–Ω—è–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –Ω–∞ –¥–≤–æ–π–Ω—ã–µ
                    json_str = json_str.replace("'", '"')
                    
                    # 2. –£–±–∏—Ä–∞–µ–º trailing commas
                    import re
                    json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
                    
                    # 3. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±—É–ª–µ–≤—ã –∑–Ω–∞—á–µ–Ω–∏—è
                    json_str = json_str.replace('True', 'true').replace('False', 'false')
                    
                    return json.loads(json_str)
            
            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            return json.loads(text)
            
        except (json.JSONDecodeError, KeyError, UnicodeDecodeError) as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            print(f"üìÑ –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text[:200]}")
            
            # –£–º–Ω—ã–π fallback - –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ regex
            try:
                import re
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ is_correct
                is_correct_match = re.search(r'"?is_correct"?\s*:\s*(true|false)', text, re.IGNORECASE)
                is_correct = is_correct_match.group(1).lower() == 'true' if is_correct_match else False
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ confidence
                confidence_match = re.search(r'"?confidence"?\s*:\s*(\d+)', text)
                confidence = int(confidence_match.group(1)) if confidence_match else 0
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ explanation —Å —É—á–µ—Ç–æ–º Unicode
                explanation_match = re.search(r'"?explanation"?\s*:\s*"([^"]*)"', text, re.UNICODE)
                explanation = explanation_match.group(1) if explanation_match else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ"
                
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —á–µ—Ä–µ–∑ regex: correct={is_correct}, conf={confidence}")
                
                return {
                    "is_correct": is_correct,
                    "confidence": confidence,
                    "explanation": explanation
                }
                
            except Exception as regex_error:
                print(f"‚ùå Regex fallback –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {regex_error}")
                
                # –ü–æ–ª–Ω—ã–π fallback
                return {
                    "is_correct": False,
                    "confidence": 0,
                    "explanation": f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –æ—Ç–≤–µ—Ç AI. –û—Ä–∏–≥–∏–Ω–∞–ª: {text[:100]}..."
                }
    
    def _fallback_check(self, student_answer: str, correct_variants: List[str], 
                        error_message: str = "–ù–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è") -> AICheckResult:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –ò–ò (fallback)"""
        student_lower = student_answer.strip().lower()
        
        for variant in correct_variants:
            if student_lower == variant.strip().lower():
                return AICheckResult(
                    is_correct=True,
                    confidence=1.0,
                    explanation="–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (fallback)",
                    ai_provider='fallback',
                    from_cache=False
                )
        
        return AICheckResult(
            is_correct=False,
            confidence=0.0,
            explanation=f"Fallback: {error_message}",
            ai_provider='fallback',
            from_cache=False
        )
    
    def batch_check_answers(self, answers_data: List[Dict]) -> List[AICheckResult]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        """
        results = []
        
        for data in answers_data:
            result = self.check_answer(
                student_answer=data['student_answer'],
                correct_variants=data['correct_variants'],
                question_context=data.get('question_context', '')
            )
            results.append(result)
        
        return results


def quick_check_answer(student_answer: str, 
                      correct_variants: List[str],
                      provider: str = "groq",
                      api_key: Optional[str] = None) -> bool:
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        checker = AIAnswerChecker(provider=provider, api_key=api_key)
        result = checker.check_answer(student_answer, correct_variants)
        return result.is_correct and result.confidence > 0.5
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ—Ç–≤–µ—Ç–∞: {e}")
        student_lower = student_answer.strip().lower()
        return any(student_lower == v.strip().lower() for v in correct_variants)